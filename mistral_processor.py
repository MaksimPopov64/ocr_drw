"""
Enhanced Mistral OCR Processor —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ç–µ–∫—Å—Ç–∞ –∏ –¥–µ—Ç–µ–∫—Ü–∏–µ–π
"""
import os
import json
import base64
import requests
import cv2
import numpy as np
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple
import re
from dataclasses import dataclass
from enum import Enum

class DocumentType(Enum):
    """–¢–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    SERVICE_ACT = "service_act"
    INVOICE = "invoice"
    CONTRACT = "contract"
    UNKNOWN = "unknown"

@dataclass
class OCRConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è OCR"""
    use_tesseract_first: bool = True
    use_llava_fallback: bool = True
    preprocess_image: bool = True
    clean_with_llm: bool = True
    max_text_length: int = 5000
    confidence_threshold: float = 0.7
    
class EnhancedMistralOCRProcessor:
    def __init__(self, 
                 ollama_url: str = "http://localhost:11434",
                 model: str = "mistral:7b-instruct-v0.2-q4_K_M",
                 config: Optional[OCRConfig] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        """
        self.ollama_url = ollama_url
        self.model = model
        self.vision_model = "llava:7b"
        self.config = config or OCRConfig()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
        self.check_ollama_connection()
        
        # –°–ª–æ–≤–∞—Ä—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π —Ç–∏–ø–∏—á–Ω—ã—Ö –æ—à–∏–±–æ–∫ OCR
        self.ocr_corrections = {
            # –†—É—Å—Å–∫–∏–µ —Å–ª–æ–≤–∞
            "–Ω–∏–æ–ø–æ–¥–ø–∏—Å–æ–Ω—Å—è": "–Ω–∏–∂–µ–ø–æ–¥–ø–∏—Å–∞–≤—à–∏–µ—Å—è",
            "–≤–ø–∞—Ä–∏—Ç—å": "–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ",
            "–≤—ã–ø–æ–ø–Ω–∏–ª": "–≤—ã–ø–æ–ª–Ω–∏–ª",
            "BRT": "–ê–ö–¢",
            "–ø—Ä–∞–¥–µ—Ç–æ–≤–∏—Ç–µ–ª–µ–º": "–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª–µ–º",
            "Boiron": "–í—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ",
            "Cyerarmum": "–°–µ—Ä–≤–∏—Å–Ω—ã–µ",
            "–≠—Ä–≤–µ": "–ó–∞–º–µ–Ω–∞",
            
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –Ω–∞ —Ä—É—Å—Å–∫–∏–µ
            "doraron": "",
            "aos yy eae": "",
            "nia wa": "–û–û–û",
            "taore Vonwrera": "–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å –ó–∞–∫–∞–∑—á–∏–∫–∞",
            "tenner": "–∫–∞—Ä—Ç—Ä–∏–¥–∂",
            
            # –ú–æ–¥–µ–ª–∏ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è
            "Ls –û–ú–ó –û–õ–ê": "LaserJet M1132",
        }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        self.extraction_patterns = {
            "claim_number": [
                r"–∑–∞—è–≤–∫\w*\s*(?:‚Ññ|N|No|#)?\s*(\d{5,})",
                r"(?:‚Ññ|N|No|#)\s*(\d{6,})",
                r"–ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏[:\s]+(\d+)",
                r"–ê–ö–¢.*?(\d{6,})"
            ],
            "equipment_model": [
                r"(HP|Canon|Xerox|Brother|Samsung|Kyocera)[\s\w]+\d+",
                r"–º–æ–¥–µ–ª—å[:\s]+([\w\s\d]+)",
                r"–ø—Ä–∏–Ω—Ç–µ—Ä[:\s]+([\w\s\d]+)",
                r"–∞–ø–ø–∞—Ä–∞—Ç[:\s]+([\w\s\d]+)"
            ],
            "cartridge_model": [
                r"(CE\d{3}[A-Z])",
                r"(Q\d{4}[A-Z])",
                r"–∫–∞—Ä—Ç—Ä–∏–¥–∂[:\s]+([\w\d]+)",
                r"(TK-\d+)",
                r"(MLT-\w\d+)"
            ],
            "customer_name": [
                r"–û–û–û\s+[\"¬´]([^\"¬ª]+)[\"¬ª]",
                r"–ó–∞–∫–∞–∑—á–∏–∫[:\s]+([^\n]+)",
                r"–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è[:\s]+([^\n]+)"
            ]
        }
    
    def advanced_preprocess_image(self, image_path: str) -> str:
        """
        –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è OCR
        """
        img = cv2.imread(image_path)
        if img is None:
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}")
        
        # 1. –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã (–µ—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —Å—Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—Ä–æ–≤–∞–Ω –ø–æ–¥ —É–≥–ª–æ–º)
        img = self.correct_perspective(img)
        
        # 2. –£–¥–∞–ª–µ–Ω–∏–µ —à—É–º–∞
        img = cv2.fastNlMeansDenoisingColored(img, None, 10, 10, 7, 21)
        
        # 3. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ grayscale
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # 4. –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è (–ª—É—á—à–µ –¥–ª—è —Ç–µ–∫—Å—Ç–∞)
        binary = cv2.adaptiveThreshold(
            gray, 255, 
            cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
            cv2.THRESH_BINARY, 
            11, 2
        )
        
        # 5. –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
        kernel = np.ones((1, 1), np.uint8)
        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
        binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
        
        # 6. –£–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–µ–∑–∫–æ—Å—Ç–∏
        kernel_sharp = np.array([[-1,-1,-1],
                                 [-1, 9,-1],
                                 [-1,-1,-1]])
        sharp = cv2.filter2D(binary, -1, kernel_sharp)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        temp_path = f"temp_enhanced_{os.path.basename(image_path)}"
        cv2.imwrite(temp_path, sharp)
        
        return temp_path
    
    def correct_perspective(self, img: np.ndarray) -> np.ndarray:
        """
        –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
        """
        try:
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            edges = cv2.Canny(gray, 50, 150, apertureSize=3)
            
            # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω—Ç—É—Ä—ã
            contours, _ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
            contours = sorted(contours, key=cv2.contourArea, reverse=True)[:5]
            
            for contour in contours:
                peri = cv2.arcLength(contour, True)
                approx = cv2.approxPolyDP(contour, 0.02 * peri, True)
                
                if len(approx) == 4:
                    # –ù–∞—à–ª–∏ —á–µ—Ç—ã—Ä–µ—Ö—É–≥–æ–ª—å–Ω–∏–∫ - –≤–æ–∑–º–æ–∂–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç
                    pts = approx.reshape(4, 2)
                    rect = self.order_points(pts)
                    
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã
                    dst = self.four_point_transform(img, rect)
                    return dst
            
            return img  # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª
            
        except Exception as e:
            print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—É: {e}")
            return img
    
    def order_points(self, pts):
        """–£–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–Ω–∏–µ —Ç–æ—á–µ–∫ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã"""
        rect = np.zeros((4, 2), dtype="float32")
        s = pts.sum(axis=1)
        rect[0] = pts[np.argmin(s)]
        rect[2] = pts[np.argmax(s)]
        diff = np.diff(pts, axis=1)
        rect[1] = pts[np.argmin(diff)]
        rect[3] = pts[np.argmax(diff)]
        return rect
    
    def four_point_transform(self, image, pts):
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã –ø–æ 4 —Ç–æ—á–∫–∞–º"""
        rect = self.order_points(pts)
        (tl, tr, br, bl) = rect
        
        widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))
        widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))
        maxWidth = max(int(widthA), int(widthB))
        
        heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))
        heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))
        maxHeight = max(int(heightA), int(heightB))
        
        dst = np.array([
            [0, 0],
            [maxWidth - 1, 0],
            [maxWidth - 1, maxHeight - 1],
            [0, maxHeight - 1]], dtype="float32")
        
        M = cv2.getPerspectiveTransform(rect, dst)
        warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))
        
        return warped
    
    def enhanced_tesseract_ocr(self, image_path: str) -> Tuple[str, float]:
        """
        –£–ª—É—á—à–µ–Ω–Ω—ã–π Tesseract OCR —Å confidence score
        """
        try:
            import pytesseract
            from PIL import Image
            
            img = Image.open(image_path)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ä–µ–∂–∏–º—ã PSM –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            psm_modes = [3, 6, 11, 4]  # –†–∞–∑–Ω—ã–µ —Ä–µ–∂–∏–º—ã —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            best_text = ""
            best_confidence = 0
            
            for psm in psm_modes:
                try:
                    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å confidence
                    custom_config = f'--psm {psm} --oem 3'
                    data = pytesseract.image_to_data(
                        img, 
                        lang='rus+eng',
                        config=custom_config,
                        output_type=pytesseract.Output.DICT
                    )
                    
                    # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –∏ —Å—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω—é—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                    words = []
                    confidences = []
                    
                    for i in range(len(data['text'])):
                        if int(data['conf'][i]) > 0:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–æ–≤–∞ –±–µ–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                            word = data['text'][i].strip()
                            if word:
                                words.append(word)
                                confidences.append(int(data['conf'][i]))
                    
                    text = ' '.join(words)
                    avg_confidence = np.mean(confidences) if confidences else 0
                    
                    if avg_confidence > best_confidence:
                        best_text = text
                        best_confidence = avg_confidence
                        
                except Exception:
                    continue
            
            print(f"‚úì Tesseract: {len(best_text)} —Å–∏–º–≤–æ–ª–æ–≤, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {best_confidence:.1f}%")
            return best_text, best_confidence / 100
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ Tesseract: {e}")
            return "", 0.0
    
    def smart_text_cleaning(self, text: str) -> str:
        """
        –£–º–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–ª–æ–≤–∞—Ä—è –∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        """
        if not text:
            return text
        
        # 1. –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–ª–æ–≤–∞—Ä—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        for wrong, correct in self.ocr_corrections.items():
            text = text.replace(wrong, correct)
        
        # 2. –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏, —Å–æ—Å—Ç–æ—è—â–∏–µ —Ç–æ–ª—å–∫–æ –∏–∑ –º—É—Å–æ—Ä–∞
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                cleaned_lines.append('')
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –±—É–∫–≤ –∫ –æ–±—â–µ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–∏–º–≤–æ–ª–æ–≤
            if len(line) > 3:
                letter_count = sum(1 for c in line if c.isalpha())
                ratio = letter_count / len(line)
                
                # –ï—Å–ª–∏ –º–µ–Ω–µ–µ 30% –±—É–∫–≤ - –≤–µ—Ä–æ—è—Ç–Ω–æ –º—É—Å–æ—Ä
                if ratio < 0.3:
                    continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –º—É—Å–æ—Ä–∞
            garbage_patterns = [
                r'^[a-z]{2,4}\s+[a-z]{2,4}\s+[a-z]{2,4}$',  # –ö–æ—Ä–æ—Ç–∫–∏–µ –∞–Ω–≥–ª —Å–ª–æ–≤–∞
                r'^[\W_]+$',  # –¢–æ–ª—å–∫–æ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã
                r'^[a-z\s]+$' if len(line) < 10 else None,  # –ö–æ—Ä–æ—Ç–∫–∏–µ –∞–Ω–≥–ª —Å—Ç—Ä–æ–∫–∏
            ]
            
            is_garbage = False
            for pattern in garbage_patterns:
                if pattern and re.match(pattern, line.lower()):
                    is_garbage = True
                    break
            
            if not is_garbage:
                cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def extract_key_information(self, text: str) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        """
        result = {
            "claim_number": None,
            "equipment_model": None,
            "cartridge_model": None,
            "customer_name": None,
            "work_type": None,
            "service_date": None
        }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        for field, patterns in self.extraction_patterns.items():
            for pattern in patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    result[field] = match.group(1).strip()
                    break
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ä–∞–±–æ—Ç
        work_types = {
            "–ó–∞–º–µ–Ω–∞ –∫–∞—Ä—Ç—Ä–∏–¥–∂–∞": ["–∑–∞–º–µ–Ω", "–∫–∞—Ä—Ç—Ä–∏–¥–∂"],
            "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ": ["–¢–û", "–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ", "–ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞"],
            "–†–µ–º–æ–Ω—Ç": ["—Ä–µ–º–æ–Ω—Ç", "–ø–æ—á–∏–Ω–∫–∞", "–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ"],
            "–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞": ["–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞", "–æ—Å–º–æ—Ç—Ä", "–ø—Ä–æ–≤–µ—Ä–∫–∞"]
        }
        
        text_lower = text.lower()
        for work_type, keywords in work_types.items():
            if any(keyword in text_lower for keyword in keywords):
                result["work_type"] = work_type
                break
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É
        date_patterns = [
            r'(\d{1,2}[./]\d{1,2}[./]\d{2,4})',
            r'(\d{1,2}\s+\w+\s+\d{4})',
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                result["service_date"] = match.group(1)
                break
        
        return result
    
    def hybrid_ocr_strategy(self, image_path: str) -> str:
        """
        –ì–∏–±—Ä–∏–¥–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è OCR: –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Ç–æ–¥–æ–≤
        """
        results = []
        
        # 1. Tesseract –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
        if self.config.use_tesseract_first:
            text, confidence = self.enhanced_tesseract_ocr(image_path)
            if text and confidence > self.config.confidence_threshold:
                results.append((text, confidence, "tesseract_original"))
        
        # 2. Tesseract –Ω–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
        if self.config.preprocess_image:
            try:
                processed_path = self.advanced_preprocess_image(image_path)
                text, confidence = self.enhanced_tesseract_ocr(processed_path)
                if text and confidence > self.config.confidence_threshold:
                    results.append((text, confidence, "tesseract_processed"))
                os.remove(processed_path)
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
        
        # 3. LLaVA –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
        if self.config.use_llava_fallback and (not results or max(r[1] for r in results) < 0.5):
            try:
                llava_text = self.extract_text_with_llava_enhanced(image_path)
                if llava_text:
                    results.append((llava_text, 0.6, "llava"))  # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è confidence –¥–ª—è LLaVA
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ LLaVA: {e}")
        
        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if results:
            best_result = max(results, key=lambda x: (x[1], len(x[0])))
            print(f"üìä –í—ã–±—Ä–∞–Ω –º–µ—Ç–æ–¥: {best_result[2]} (confidence: {best_result[1]:.2f})")
            return best_result[0]
        
        return ""
    
    def extract_text_with_llava_enhanced(self, image_path: str) -> str:
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ LLaVA
        """
        try:
            image_base64 = self.encode_image_to_base64(image_path)
            
            # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            prompt = """You are an expert OCR system for Russian service documents.
Analyze the image and extract the data into a structured format.

Focus on:
1. Service Act Number (–ê–ö–¢ –ø–æ –∑–∞—è–≤–∫–µ ‚Ññ)
2. Equipment Model (–ú–æ–¥–µ–ª—å –∞–ø–ø–∞—Ä–∞—Ç–∞)
3. Serial Number (–°–µ—Ä–∏–π–Ω—ã–π ‚Ññ)
4. Counter readings (–°—á–µ—Ç—á–∏–∫ —Å—Ç—Ä–∞–Ω–∏—Ü: –ß/–ë and –¶–≤–µ—Ç–Ω—ã—Ö)
5. COMPLETED WORKS (–í—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ —Ä–∞–±–æ—Ç—ã) - extract as a list of items with descriptions and quantities if available.
6. Checkboxes - indicate which specific works were checked (–û—Å–º–æ—Ç—Ä, –ò–Ω—Å—Ç–∞–ª–ª—è—Ü–∏—è, –¢–û1, –¢–û2, –¢–û3, –†–µ–º–æ–Ω—Ç, –î–æ—Å—Ç–∞–≤–∫–∞).

FORMAT: Return as a clean JSON object.
{
  "act_number": "number",
  "equipment": {
    "model": "model name",
    "serial": "serial number",
    "counters": {"bw": 0, "color": 0}
  },
  "work_items": [
    {"description": "item description", "quantity": 1}
  ],
  "checkboxes": {
    "inspection": boolean,
    "installation": boolean,
    "to1": boolean,
    "to2": boolean,
    "to3": boolean,
    "repair": boolean,
    "delivery": boolean
  }
}
If JSON is not possible, extract text maintaining layout."""
            
            payload = {
                "model": "llava:7b",
                "prompt": prompt,
                "images": [image_base64],
                "stream": False,
                "options": {
                    "temperature": 0.05,
                    "num_predict": 4096,
                    "seed": 42
                }
            }
            
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=300
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "").strip()
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ LLaVA: {e}")
        
        return ""
    
    def advanced_llm_cleaning(self, text: str) -> str:
        """
        –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM
        """
        if not text or len(text.strip()) < 10:
            return text
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–∏–º–µ–Ω—è–µ–º –±—ã—Å—Ç—Ä—É—é –æ—á–∏—Å—Ç–∫—É
        text = self.smart_text_cleaning(text)
        
        if not self.config.clean_with_llm:
            return text
        
        try:
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è LLM
            text_chunk = text[:2000] if len(text) > 2000 else text
            
            prompt = f"""You are a Russian document text correction expert.

INPUT: OCR text from a Russian service document with recognition errors.

YOUR TASK:
1. Fix OCR errors in Russian words
2. Remove garbage text that doesn't make sense
3. Reconstruct damaged Russian words
4. Keep all numbers, dates, and model names intact
5. Preserve document structure

COMMON CORRECTIONS:
- "–Ω–∏–æ–ø–æ–¥–ø–∏—Å–æ–Ω—Å—è" ‚Üí "–Ω–∏–∂–µ–ø–æ–¥–ø–∏—Å–∞–≤—à–∏–µ—Å—è"
- "–≤—ã–ø–æ–ø–Ω–∏–ª" ‚Üí "–≤—ã–ø–æ–ª–Ω–∏–ª"
- "BRT" ‚Üí "–ê–ö–¢"
- Random English letters between Russian words should be removed

CORRUPTED TEXT:
{text_chunk}

CORRECTED TEXT (Russian, clean, structured):"""
            
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,
                        "top_p": 0.9,
                        "num_predict": 2048
                    }
                },
                timeout=60
            )
            
            if response.status_code == 200:
                cleaned = response.json().get("response", text).strip()
                if cleaned and len(cleaned) > len(text) * 0.3:  # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –Ω–µ —É–¥–∞–ª–∏–ª–∏ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –æ—Å—Ç–∞–ª—å–Ω–æ–π —á–∞—Å—Ç—å—é –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –±—ã–ª –æ–±—Ä–µ–∑–∞–Ω
                    if len(text) > 2000:
                        return cleaned + text[2000:]
                    return cleaned
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ LLM –æ—á–∏—Å—Ç–∫–∏: {e}")
        
        return text
    
    def detect_signature_and_stamp_advanced(self, image_path: str) -> Dict[str, bool]:
        """
        –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–æ–¥–ø–∏—Å–∏ –∏ –ø–µ—á–∞—Ç–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç CV-–º–µ—Ç–æ–¥—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã—Ö –∏ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
        """
        try:
            img = cv2.imread(image_path)
            if img is None:
                return {"has_signature": False, "has_stamp": False}
            
            height, width = img.shape[:2]
            
            # 1. –ü–æ–∏—Å–∫ –ø–µ—á–∞—Ç–∏ (–∫—Ä—É–≥–∏ + —Ü–≤–µ—Ç)
            hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
            
            # –î–∏–∞–ø–∞–∑–æ–Ω—ã –¥–ª—è –∫—Ä–∞—Å–Ω–æ–≥–æ –∏ —Å–∏–Ω–µ–≥–æ (—Ç–∏–ø–∏—á–Ω—ã–µ —Ü–≤–µ—Ç–∞ –ø–µ—á–∞—Ç–µ–π)
            red_mask = cv2.bitwise_or(
                cv2.inRange(hsv, np.array([0, 50, 50]), np.array([10, 255, 255])),
                cv2.inRange(hsv, np.array([170, 50, 50]), np.array([180, 255, 255]))
            )
            blue_mask = cv2.inRange(hsv, np.array([100, 50, 50]), np.array([130, 255, 255]))
            color_mask = cv2.bitwise_or(red_mask, blue_mask)
            
            # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞
            kernel = np.ones((5, 5), np.uint8)
            color_mask = cv2.morphologyEx(color_mask, cv2.MORPH_OPEN, kernel)
            
            # –ü–æ–∏—Å–∫ –∫—Ä—É–≥–ª—ã—Ö –∫–æ–Ω—Ç—É—Ä–æ–≤
            contours, _ = cv2.findContours(color_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            has_stamp = False
            for cnt in contours:
                area = cv2.contourArea(cnt)
                if area > 1000:
                    perimeter = cv2.arcLength(cnt, True)
                    circularity = 4 * np.pi * area / (perimeter * perimeter) if perimeter > 0 else 0
                    if circularity > 0.5:
                        has_stamp = True
                        break
            
            # 2. –ü–æ–∏—Å–∫ –ø–æ–¥–ø–∏—Å–∏ (–Ω–∏–∂–Ω—è—è —Ç—Ä–µ—Ç—å, –≤—ã—Å–æ–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –ª–∏–Ω–∏–π)
            bottom_start = int(height * 0.7)
            bottom_area = img[bottom_start:height, :]
            gray_bottom = cv2.cvtColor(bottom_area, cv2.COLOR_BGR2GRAY)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Canny –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫—Ä–∞–µ–≤ (–ø–æ–¥–ø–∏—Å–∏ –æ–±—ã—á–Ω–æ –∏–º–µ—é—Ç –º–Ω–æ–≥–æ —Ä–µ–∑–∫–∏—Ö –∫—Ä–∞–µ–≤)
            edges = cv2.Canny(gray_bottom, 50, 150)
            edge_density = np.sum(edges > 0) / (edges.shape[0] * edges.shape[1])
            
            # –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ —Ä—É–∫–æ–ø–∏—Å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            has_signature = edge_density > 0.01
            
            return {
                "has_signature": has_signature,
                "has_stamp": has_stamp
            }
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ detect_signature_and_stamp_advanced: {e}")
            return {"has_signature": False, "has_stamp": False}

    def process_document_enhanced(self, image_path: str, expected_claim_number: Optional[str] = None) -> Dict[str, Any]:
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        """
        start_time = datetime.now()
        print(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {image_path}")
        
        try:
            # 1. –ì–∏–±—Ä–∏–¥–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
            print("üìñ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ (–≥–∏–±—Ä–∏–¥–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è)...")
            extracted_text = self.hybrid_ocr_strategy(image_path)
            
            if not extracted_text:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
            
            print(f"üìä –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(extracted_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # 2. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ—á–∏—Å—Ç–∫–∞
            print("üßπ –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞...")
            cleaned_text = self.advanced_llm_cleaning(extracted_text)
            print(f"üìä –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(cleaned_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # 3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            print("üîë –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏...")
            key_info = self.extract_key_information(cleaned_text)
            
            # 4. –î–µ—Ç–µ–∫—Ü–∏—è –ø–æ–¥–ø–∏—Å–∏ –∏ –ø–µ—á–∞—Ç–∏
            print("üñãÔ∏è –ü–æ–∏—Å–∫ –ø–æ–¥–ø–∏—Å–∏ –∏ –ø–µ—á–∞—Ç–∏...")
            signature_stamp = self.detect_signature_and_stamp_advanced(image_path)
            
            # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
            print("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π...")
            check_result = self.check_requirements_enhanced(
                key_info, 
                signature_stamp, 
                expected_claim_number
            )
            
            # 6. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_type = self.detect_document_type(cleaned_text)
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            result = {
                "success": True,
                "timestamp": datetime.now().isoformat(),
                "filename": os.path.basename(image_path),
                "processing_time_seconds": processing_time,
                "document_type": doc_type.value,
                "extracted_data": {
                    **key_info,
                    "has_signature": signature_stamp.get("has_signature", False),
                    "has_stamp": signature_stamp.get("has_stamp", False),
                    "text_preview": cleaned_text[:500] + "..." if len(cleaned_text) > 500 else cleaned_text
                },
                "validation": check_result,
                "full_text": cleaned_text,
                "metadata": {
                    "ocr_engine": "Hybrid (Tesseract + LLaVA)",
                    "llm_model": self.model,
                    "preprocessing_applied": self.config.preprocess_image,
                    "llm_cleaning_applied": self.config.clean_with_llm
                }
            }
            
            print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {processing_time:.2f} —Å–µ–∫")
            print(f"üìã –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞: {doc_type.value}")
            print(f"üìù –ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏: {key_info.get('claim_number', '–ù–µ –Ω–∞–π–¥–µ–Ω')}")
            
            return result
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "filename": os.path.basename(image_path)
            }
    
    def detect_document_type(self, text: str) -> DocumentType:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        """
        text_lower = text.lower()
        
        if "–∞–∫—Ç" in text_lower and "–∑–∞—è–≤–∫" in text_lower:
            return DocumentType.SERVICE_ACT
        elif "—Å—á–µ—Ç" in text_lower or "invoice" in text_lower:
            return DocumentType.INVOICE
        elif "–¥–æ–≥–æ–≤–æ—Ä" in text_lower or "contract" in text_lower:
            return DocumentType.CONTRACT
        else:
            return DocumentType.UNKNOWN
    
    def check_requirements_enhanced(self, 
                                   key_info: Dict, 
                                   signature_stamp: Dict,
                                   expected_claim: Optional[str]) -> Dict:
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
        """
        issues = []
        warnings = []
        suggestions = []
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–º–µ—Ä–∞ –∑–∞—è–≤–∫–∏
        if expected_claim:
            claim = key_info.get("claim_number")
            if claim:
                if str(claim) != str(expected_claim):
                    issues.append({
                        "field": "claim_number",
                        "issue": "mismatch",
                        "expected": expected_claim,
                        "found": claim,
                        "severity": "high"
                    })
            else:
                warnings.append({
                    "field": "claim_number",
                    "issue": "not_found",
                    "severity": "medium"
                })
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
        required_fields = {
            "equipment_model": "–ú–æ–¥–µ–ª—å –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è",
            "customer_name": "–ù–∞–∑–≤–∞–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫–∞",
            "work_type": "–¢–∏–ø –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö —Ä–∞–±–æ—Ç"
        }
        
        for field, description in required_fields.items():
            if not key_info.get(field):
                warnings.append({
                    "field": field,
                    "issue": "missing",
                    "description": f"{description} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ",
                    "severity": "medium"
                })
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–ø–∏—Å–∏ –∏ –ø–µ—á–∞—Ç–∏
        if not signature_stamp.get("has_signature"):
            issues.append({
                "field": "signature",
                "issue": "missing",
                "description": "–ü–æ–¥–ø–∏—Å—å –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞",
                "severity": "high"
            })
        
        if not signature_stamp.get("has_stamp"):
            warnings.append({
                "field": "stamp",
                "issue": "missing",
                "description": "–ü–µ—á–∞—Ç—å –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞",
                "severity": "low"
            })
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        if issues:
            suggestions.append("–î–æ–∫—É–º–µ–Ω—Ç —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏ –ø–µ—Ä–µ–¥ –∑–∞–∫—Ä—ã—Ç–∏–µ–º –∑–∞—è–≤–∫–∏")
        elif warnings:
            suggestions.append("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
        else:
            suggestions.append("–î–æ–∫—É–º–µ–Ω—Ç –≥–æ—Ç–æ–≤ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞
        if any(i["severity"] == "high" for i in issues):
            status = "REJECTED"
        elif warnings:
            status = "NEEDS_REVIEW"
        else:
            status = "APPROVED"
        
        return {
            "status": status,
            "issues": issues,
            "warnings": warnings,
            "suggestions": suggestions,
            "can_process": status == "APPROVED",
            "requires_manual_review": status == "NEEDS_REVIEW"
        }

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
    config = OCRConfig(
        use_tesseract_first=True,
        use_llava_fallback=True,
        preprocess_image=True,
        clean_with_llm=True,
        confidence_threshold=0.5
    )
    
    processor = EnhancedMistralOCRProcessor(config=config)
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
    result = processor.process_document_enhanced(
        "path/to/your/document.jpg",
        expected_claim_number="1847896"
    )
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    if result["success"]:
        print("\nüìÑ –†–ï–ó–£–õ–¨–¢–ê–¢ –û–ë–†–ê–ë–û–¢–ö–ò:")
        print(f"–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞: {result['document_type']}")
        print(f"–°—Ç–∞—Ç—É—Å –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {result['validation']['status']}")
        print(f"\n–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:")
        for key, value in result['extracted_data'].items():
            if value and key != 'text_preview':
                print(f"  ‚Ä¢ {key}: {value}")
    else:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {result['error']}")
